# DecentDB Performance Review
**Date:** 2026-01-28
**Reviewer:** AG-G3PRO
**Scope:** Core Engine, Pager, B-Tree, WAL, Planner

## Executive Summary

DecentDB demonstrates a functional database architecture with standard components (WAL, Pager, B-Tree). However, the current implementation contains critical performance bottlenecks and resource management issues that will severely limit scalability and reliability in production environments.

The most critical issues are:
1.  **Unbounded Memory Usage in WAL**: The Write-Ahead Log keeps full copies of all dirty pages in RAM.
2.  **O(N) Complexity for Mutations**: `UPDATE` and `DELETE` operations always perform full table scans.
3.  **Data Size Limitations**: Lack of overflow page support prevents storing rows larger than ~4KB.

## Detailed Findings

### 1. Write-Ahead Log (WAL) Memory Management
**Severity:** Critical
**Location:** `src/wal/wal.nim`

The WAL implementation maintains an in-memory index (`wal.index`) that mapping `PageId` to `seq[WalIndexEntry]`. Crucially, `WalIndexEntry` stores the **entire page payload** (4KB) in a byte sequence.

*   **Impact**: Memory usage grows linearly with the write volume. A transaction or series of transactions modifying 1GB of data will consume >1GB of RAM. This effectively negates the benefit of a disk-based database for write-heavy workloads.
*   **Recommendation**: 
    *   Modify `WalIndexEntry` to store only the `offset` of the frame in the WAL file, not the payload.
    *   Read the page content from the WAL file on demand during `readPageWithSnapshot`.

### 2. Mutation Performance (UPDATE/DELETE)
**Severity:** Critical
**Location:** `src/engine.nim` (`skUpdate` and `skDelete` handlers)

Both `UPDATE` and `DELETE` operations explicitly call `scanTable(db.pager, table)`, iterating through every row in the table to evaluate the `WHERE` clause.

*   **Impact**: Performance degrades linearly with table size (O(N)). Deleting a single row by Primary Key in a table of 1 million rows will require reading and parsing 1 million rows.
*   **Recommendation**: 
    *   Integrate the `planner` module into the update/delete execution paths.
    *   Use `pkIndexSeek` or other index strategies to identify target row IDs before modifying them.

### 3. B-Tree Implementation Limits
**Severity:** High
**Location:** `src/btree/btree.nim`

#### A. No Overflow Support
The B-Tree implementation returns `ERR_IO "Leaf overflow"` or `"Internal overflow"` if the keys/values + metadata exceed the page size (4KB).

*   **Impact**: Users cannot store rows larger than a single page. This makes the database unsuitable for storing blobs, long text, or wide tables.
*   **Recommendation**: Implement overflow pages (linked list of pages) to handle values that exceed the B-Tree node capacity.

#### B. Linear Search within Nodes
The `findLeaf` and `find` procedures perform linear scans over keys inside B-Tree nodes.

*   **Code Reference**: 
    ```nim
    for i in 0 ..< keys.len:
      if key < keys[i]: ...
    ```
*   **Impact**: unnecessary CPU overhead (O(N)) per node traversal. While N is small (limited by page size), binary search (O(log N)) is standard and significantly faster for CPU-bound lookups.
*   **Recommendation**: Replace linear scans with binary search algorithms.

#### C. Excessive Copying
`encodeLeaf` and `insertRecursive` re-create and copy the entire node content on every insert/update.

*   **Impact**: High CPU and memory bandwidth usage during writes.

### 4. Query Planner Limitations
**Severity:** Medium
**Location:** `src/planner/planner.nim`

The planner only recognizes strict equality (`=`) for index usage (`pkIndexSeek`).

*   **Impact**: Range queries (`>`, `<`, `>=`, `<=`, `BETWEEN`) will force full table scans, rendering indexes useless for time-series data or pagination tasks.
*   **Recommendation**: Extend `planSelect` to recognize range operators and map them to B-Tree range scans.

### 5. Pager Locking and IO
**Severity:** Medium
**Location:** `src/pager/pager.nim`

`flushAll` acquires the global cache lock for the entire duration of determining dirty pages, writing them to disk (synchronously), and calling `fsync`.

*   **Impact**: `flushAll` (called during checkpoints and commit) causes a "stop-the-world" pause. No other readers or writers can access the cache during this time.
*   **Recommendation**: 
    *   Implement dirty page gathering under lock, but perform disk I/O outside the main lock if possible (though difficult with strict serializability).
    *   Consider a background writer to trickle-flush pages.

### 6. Concurrency Risks
**Severity:** High
**Location:** `src/wal/wal.nim`

There appears to be a race condition between `commit` (Writer) and `readPageWithSnapshot` (Reader).
*   `commit` modifies `wal.index` sequences by appending entries.
*   `getPageAtOrBefore` reads these sequences.
*   While `commit` holds `wal.lock`, `getPageAtOrBefore` does **not** appear to acquire `wal.lock`. It relies on proper memory ordering, but Nim's `Table` and `seq` are not concurrent-safe. If `commit` triggers a sequence reallocation, readers could segfault or read garbage.

## Conclusion

The current implementation is a "Proof of Concept" quality engine. To achieve "decent" performance and reliability, the rewrite of the WAL memory model and the integration of indexes for mutations are prerequisites.
