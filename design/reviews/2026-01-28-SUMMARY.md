# 2026-01-28 Consolidated Review Summary (Ranked Task List)

This document consolidates findings across the review documents in `design/reviews/` and turns them into a single, deduplicated task list.

## Inputs (reviews consolidated)
- `design/reviews/2026-01-28-AG-G3PRO.md`
- `design/reviews/2026-01-28-DEEPSEEk32.md`
- `design/reviews/2026-01-28-GROKCODEFAST.md`
- `design/reviews/2026-01-28-KIMIK25.md`
- `design/reviews/2026-01-28-MIMOV2.md`
- `design/reviews/2026-01-28-QWEN.md`

## Ranking method
Each item below includes:
- **Frequency**: “Seen in X/6 reviews” (higher ranks earlier within a priority tier).
- **Priority** (impact-driven):
  - **P0**: MVP blockers (correctness/durability or “cannot run target workloads”)
  - **P1**: High-impact performance / scalability work needed to meet PRD targets
  - **P2**: Medium impact or targeted optimizations
  - **P3**: Post-MVP / nice-to-have (or “needs ADR” before implementation)

Within each priority tier, items are ordered by **frequency**, then by estimated impact.

## Task list (deduplicated, ranked)

### P0 — MVP blockers

- [x] **External sort: remove the hard spill run-count limit by implementing multi-pass merge (bounded fan-in)** (Seen in 4/6: `MIMOV2`, `KIMIK25`, `GROKCODEFAST`, `QWEN`)
  - **Problem:** The previous external sort path could fail once the number of spill runs exceeded a fixed limit (commonly cited as “64 runs”), even though the input could be processed in multiple merge passes.
  - **Impact:** Sorting large-but-valid datasets could fail outright instead of degrading by I/O cost.
  - **Actions completed:**
    - Implemented multi-pass merging: merge runs in groups up to the open-run limit, repeat until the final merge completes.
    - Removed the “exceeded spill run limit” failure mode.
    - Added coverage: a unit test that forces multiple merge passes with small buffer/open-run settings.

- [x] **Add a hard policy for long-running readers to prevent unbounded WAL growth (disk and memory)** (Seen in 4/6: `MIMOV2`, `KIMIK25`, `GROKCODEFAST`, `DEEPSEEk32`)
  - **Problem:** Snapshot readers can pin old WAL frames. With long-lived readers, the WAL can grow indefinitely, preventing truncation/checkpoint progress (per ADR-0019), leading to resource exhaustion.
  - **Impact:** Real deployments can hit disk exhaustion (WAL never truncates) or memory exhaustion (depending on how many versions are retained in-memory), creating “works in tests, dies in production” failure modes.
  - **Actions completed:**
    - Implemented reader identity tracking (`ReadTxn`) and safe timeout handling: timed-out readers are aborted and removed from the active-reader registry.
    - Added a read-guard hook so aborted readers fail fast with a transaction error rather than silently reading an inconsistent view.
    - Enabled a conservative default policy (checkpoint-by-size + warn/timeout thresholds) and added CLI flags to override configuration.
    - Ensured checkpoints can proceed after aborting timed-out readers (prevents indefinite pinning).
  - **Validation / tests:**
    - Updated/added unit coverage for reader warnings and forced truncation behavior.

- [x] **Fix index seeks that devolve to O(n) scans; ensure point lookups are O(log n) end-to-end** (Seen in 3/6: `KIMIK25`, `AG-G3PRO`, `DEEPSEEk32`)
  - **Problem:** Several reviews flag “index lookup behaving like a full scan”:
    - `KIMIK25`: `indexSeek` scans the entire B+Tree cursor instead of using a logarithmic `find`/seek.
    - `AG-G3PRO` / `DEEPSEEk32`: additional linear scans within B+Tree nodes/pages are highlighted as avoidable overhead.
  - **Impact:** Anything relying on indexes (PK lookups, FK joins, uniqueness checks) can become catastrophically slow at scale (millions of rows), violating PRD P95 targets.
  - **Actions completed:**
    - Added `openCursorAt` (seek to first key >= target) and updated `indexSeek`/`indexHas*` helpers to use it, avoiding full cursor scans.
    - Updated B+Tree leaf insertion and index maintenance to correctly support duplicate keys for non-unique indexes.
    - Fixed index maintenance for UPDATE/DELETE to delete the specific `(key,rowid)` entry rather than deleting “some row” for that key.
  - **Validation / tests:**
    - Added/expanded unit coverage to ensure FK RESTRICT remains correct with multiple children sharing the same parent key.

- [x] **Stop storing full page images in the in-memory WAL index; make WAL page retrieval on-demand** (Seen in 2/6: `AG-G3PRO`, `KIMIK25`)
  - **Actions completed:**
    - `WalIndexEntry` now stores WAL frame offsets (not full page payloads).
    - Snapshot reads load page images from the WAL file on demand.

- [x] **Make WAL reader/writer concurrency safe (eliminate races accessing `wal.index` / sequences)** (Seen in 1/6 explicitly: `AG-G3PRO`; related locking concerns in others)
  - **Problem:** `AG-G3PRO` calls out a likely race where a writer mutates `wal.index` and underlying `seq` storage while a reader reads without holding a lock; Nim `Table`/`seq` are not safe for concurrent mutation + read.
  - **Impact:** This is a correctness and stability risk: crashes (segfault), data corruption in memory, or readers observing “impossible” states.
  - **Actions completed:**
    - Added a dedicated lock to protect `wal.index` updates and lookups, removing concurrent `Table/seq` mutation hazards.

- [x] **Implement overflow pages for large TEXT/BLOB / wide rows (SPEC/PRD requirement)** (Seen in 1/6: `AG-G3PRO` but mandated by `design/PRD.md` + `design/SPEC.md`)
  - **Problem:** Without overflow pages, rows/values larger than a single page (~4KB at default page size) fail with overflow errors.
  - **Impact:** This blocks core MVP functionality for realistic schemas (long text, blobs, or wide tables). It also conflicts with SPEC’s “Mandatory Overflow Pages” requirement.
  - **Notes:**
    - Overflow chains are implemented and covered by unit tests (`record` overflow roundtrip; storage normalizes large values into overflow pointers).

- [x] **Make UPDATE/DELETE avoid unconditional full-table scans by reusing the planner and indexes** (Seen in 1/6: `AG-G3PRO`)
  - **Problem:** `UPDATE` and `DELETE` are reported to always scan the full table and evaluate the WHERE clause row-by-row, even when predicates target indexed columns (e.g., PK).
  - **Impact:** Basic mutations become unusable on large tables (O(N) per statement). This also makes FK/constraint-heavy workloads far slower than needed.
  - **Actions completed:**
    - Added an indexed fast path for simple equality predicates (`col = literal/param`) when a B+Tree index exists, falling back to full scans only when necessary.

### P1 — High-impact (meet PRD targets; reduce contention / major hotspots)

- [ ] **Reduce page cache contention and copying (global lock, CLOCK behavior, per-read page copies)** (Seen in 5/6: `DEEPSEEk32`, `GROKCODEFAST`, `KIMIK25`, `MIMOV2`, `QWEN`; related `AG-G3PRO` on flush locking)
  - **Problem:** Multiple reviews flag the pager/page-cache as a concurrency and throughput limiter:
    - Global cache lock serializes readers.
    - CLOCK eviction has high worst-case scan overhead and edge-case behavior concerns.
    - Snapshot reads and/or cached reads involve full-page `copyMem` per access.
    - Cache sizing defaults appear far too small for target workloads (`MIMOV2`).
  - **Impact:** Read throughput and tail latency degrade under concurrency; cache becomes a scalability ceiling, especially for B+Tree-heavy queries.
  - **Actions to resolve:**
    - Increase default cache sizing to something non-trivial for the target dataset (e.g., `MIMOV2` suggests 4MB as a baseline; ideally make it configurable and well-documented).
    - Replace the single global cache lock with sharded locks (or another reader-friendly structure) to reduce contention on hot lookups.
    - Rework snapshot/copying behavior: use copy-on-write or reference-counted immutable page buffers so “readers don’t copy 4KB by default”.
    - Harden eviction: guarantee termination when pages are pinned, and ensure eviction scanning doesn’t become O(capacity) in common cases.
  - **Validation / tests:**
    - Add concurrent read benchmark: N reader threads + a writer; assert throughput scaling and no deadlocks.
    - Add unit tests for eviction under pinned-page scenarios (must not infinite-loop).
    - Add memory regression tests for “repeated reads do not allocate per read”.

- [ ] **Unblock checkpoints: avoid full WAL-index scans and long lock holds; reduce “stop-the-world” flush behavior** (Seen in 5/6: `DEEPSEEk32`, `GROKCODEFAST`, `KIMIK25`, `MIMOV2`, `QWEN`; related `AG-G3PRO` on `flushAll`)
  - **Problem:** Checkpointing and flushing are described as potentially:
    - Iterating over all dirty/WAL-tracked pages (O(n) scans),
    - Holding global locks while doing synchronous I/O + fsync,
    - Blocking writers for long periods (`KIMIK25`, `AG-G3PRO`).
  - **Impact:** Large checkpoints cause latency spikes and stall foreground operations; this threatens both write latency and read responsiveness.
  - **Actions to resolve:**
    - Implement incremental checkpointing (track “pages to checkpoint” as a work queue rather than scanning everything).
    - Minimize lock hold times: gather work under lock, perform I/O outside lock where correctness permits.
    - Add a background checkpoint mode (if/when safe) that respects “reader protection” rules (ADR-0019).
  - **Validation / tests:**
    - Add a checkpoint stress test with concurrent readers/writer; assert writers aren’t blocked for unbounded time.
    - Add crash-injection around checkpoint boundaries to ensure durability invariants.

- [ ] **Fix join execution scalability (index nested loop for FK paths; avoid quadratic materialization)** (Seen in 5/6: `DEEPSEEk32`, `GROKCODEFAST`, `KIMIK25`, `MIMOV2`, `QWEN`)
  - **Problem:** Reviews consistently flag naive join behavior:
    - Nested loops without leveraging indexes effectively.
    - No join reordering / limited heuristics (`MIMOV2`).
    - Excessive intermediate materialization (`KIMIK25`).
  - **Impact:** Joins on realistic normalized schemas (artist→album→track) risk exploding runtime and memory; this directly threatens PRD’s “FK join < 100ms P95” goal at scale.
  - **Actions to resolve:**
    - Implement index-nested-loop join for common FK patterns (seek child index using parent row key/rowid).
    - Ensure join operators can stream outputs rather than constructing `seq[Row]` for the entire join.
    - Add simple join ordering heuristics (even without full cost-based optimization) to avoid worst-case join orders.
  - **Validation / tests:**
    - Add macrobenchmarks on synthetic FK datasets of increasing size; track scaling.
    - Add correctness tests comparing join output ordering and semantics against expected results (and differential tests vs PostgreSQL for supported subset).

- [ ] **Reduce allocation/GC pressure via buffer reuse / pools (record encoding, row materialization, temp buffers)** (Seen in 5/6: `DEEPSEEk32`, `GROKCODEFAST`, `KIMIK25`, `MIMOV2`, `QWEN`)
  - **Problem:** Multiple hot paths allocate many short-lived `seq` buffers (record encoding/decoding, page copies, row materialization), and reviews note the lack of implemented memory pooling despite ADR references.
  - **Impact:** GC pressure, latency spikes, and reduced throughput, especially for scans, joins, bulk operations, and index maintenance.
  - **Actions to resolve:**
    - Implement a page-sized buffer pool (slab/arena) for 4KB (and configurable page size) buffers.
    - Add query-scoped arenas for ephemeral allocations (rows/expressions/materialization).
    - Reduce unnecessary copying in encoding/decoding by writing into pre-sized buffers and reusing them.
  - **Validation / tests:**
    - Add allocation counters in test builds (or benchmark instrumentation) and assert reductions on representative workloads.
    - Add memory regression tests (peak RSS) for bulk operations.

- [x] **Add query plan caching / prepared statement support with schema invalidation** (Seen in 5/6: `DEEPSEEk32`, `GROKCODEFAST`, `KIMIK25`, `MIMOV2`, `QWEN`)
  - **Problem:** Repeated SQL pays parse + plan cost every time; multiple reviews call out missing plan caching/prepared statements.
  - **Impact:** Adds avoidable latency and CPU overhead for interactive workloads with repeated queries.
  - **Actions completed:**
    - Implemented an LRU cache that reuses parsed/bound statements and precomputed SELECT plans keyed by SQL text + schema cookie.
    - Cache invalidates automatically on schema changes (`schemaBump` clears the cache).

- [x] **Make constraint enforcement use indexed point lookups (UNIQUE/FK/RESTRICT) instead of scans** (Seen in 3/6: `KIMIK25`, `GROKCODEFAST`, `QWEN`)
  - **Problem:** Enforcing uniqueness and FK constraints is described as O(N) due to scanning/indexSeek patterns, and becomes much worse if index seeks are also O(N).
  - **Impact:** Inserts/updates with constraints can degrade to quadratic time, making bulk load and normal writes miss PRD targets.
  - **Actions completed:**
    - UNIQUE checks use indexed existence/“other rowid” probes instead of materializing match lists.
    - FK and RESTRICT checks use indexed existence probes on parent/child keys.

- [ ] **Harden/optimize bulk load to meet PRD targets without excessive memory use** (Seen in 3/6: `MIMOV2`, `DEEPSEEk32`, `QWEN`; related write-path concerns in `KIMIK25`)
  - **Problem:** Reviews note missing/insufficient bulk-load specialization (B+Tree bulk build, reduced split churn) and highlight high memory usage patterns during bulk load (e.g., large “seen sets” for uniqueness checks).
  - **Impact:** Risk of missing PRD bulk load target (100k records in < 20s) and risk of OOM during large loads.
  - **Actions to resolve:**
    - Implement/finish ADR-0027 bulk load API semantics and ensure it uses write-path optimizations (batch allocation, reduced per-row overhead, minimized index maintenance work).
    - Add a “sorted bulk build” path for B+Tree indexes where applicable (bottom-up build or batched inserts to reduce page splits).
    - Replace large in-memory uniqueness tracking with chunked/streaming approaches (spill-to-disk or partitioned hash strategy) where needed.
  - **Validation / tests:**
    - Add a bulk-load benchmark at 100k+ rows with configured durability modes; record throughput + peak RSS.
    - Add correctness tests ensuring constraints still hold (or are explicitly deferred and then validated, if bulk mode supports deferral).

- [ ] **Improve trigram index write-path and query heuristics (avoid full decode/re-encode; handle high-frequency trigrams)** (Seen in 3/6: `KIMIK25`, `GROKCODEFAST`, `MIMOV2`; related LIKE/index fallback concerns in `QWEN`)
  - **Problem:** Updating postings lists by decoding, modifying, and re-encoding entire lists scales poorly for common trigrams; queries can also degrade if large postings lists aren’t handled intelligently.
  - **Impact:** Text-heavy inserts/updates become prohibitively slow; substring search can regress badly on “common token” patterns.
  - **Actions to resolve:**
    - Redesign postings updates to avoid O(n) rewrite per update (e.g., delta segments + periodic merge, or append-only with compaction).
    - Add frequency-aware query planning for trigram intersection (start with rarest postings lists; apply guardrails for too-common trigrams).
    - Ensure `LIKE` fallback behavior is explicit and has guardrails (avoid silently running full scans for broad patterns without warnings).
  - **Validation / tests:**
    - Add benchmarks for inserts/updates into trigram-indexed columns with both rare and common trigrams.
    - Add correctness tests for trigram postings merge/compaction (if introduced).

### P2 — Targeted optimizations / risk hardening

- [ ] **Harden LIKE pattern matching to avoid worst-case backtracking blowups; add guardrails** (Seen in 1/6: `KIMIK25`)
  - **Problem:** The current LIKE matcher is described as a backtracking implementation with O(n×m) worst cases and potential catastrophic behavior on crafted patterns.
  - **Impact:** CPU denial-of-service style behavior (even in embedded settings, this can hang the process).
  - **Actions to resolve:**
    - Replace the backtracking matcher with a linear-time algorithm (e.g., NFA-based).
    - Add explicit complexity limits (pattern length, number of wildcards) and/or time budgets.
    - Ensure trigram prefiltering is used where available to reduce candidate rows before LIKE evaluation.
  - **Validation / tests:**
    - Add adversarial pattern tests that previously caused excessive runtime.
    - Add property tests comparing LIKE semantics to a reference implementation for supported patterns.

- [ ] **Remove single-file-handle serialization in VFS for read-heavy workloads (or document and bound it)** (Seen in 2/6: `QWEN`, `MIMOV2`)
  - **Problem:** File I/O is reported to be serialized via per-file locks/wrappers, limiting concurrent reader throughput.
  - **Impact:** Reader threads can be bottlenecked on a lock even when performing independent reads; this conflicts with “many concurrent readers”.
  - **Actions to resolve:**
    - Audit which VFS operations truly require serialization (vs. per-handle safety).
    - Where safe, use separate file handles for concurrent reads, or narrow lock scope to only critical sections (e.g., file position mutations).
    - Add metrics to quantify lock contention before/after changes.
  - **Validation / tests:**
    - Add concurrent read benchmark at varying thread counts; ensure throughput scales.

- [ ] **Add/expand large-dataset + concurrent-load benchmarks and performance regression gates** (Seen in 3/6: `DEEPSEEk32`, `GROKCODEFAST`, `MIMOV2`)
  - **Problem:** Reviews note that current benchmarks are small or incomplete for the PRD’s target dataset sizes and concurrency expectations.
  - **Impact:** Regressions (or missing scalability limits) will ship unnoticed; the system may “look fast” on 1k rows but fail at 1M+.
  - **Actions to resolve:**
    - Add benchmark variants at 1M+ scale for: point lookups, range scans, FK joins, trigram LIKE, and ORDER BY sorts.
    - Add concurrent benchmarks: N readers + single writer, plus long-reader scenarios that pin WAL.
    - Track memory and I/O counters alongside latency (peak RSS, fsync count, WAL bytes written).
  - **Validation / tests:**
    - Add CI thresholds (baseline + % regression budgets) for key benchmarks.
    - Add periodic “nightly” large-scale benchmark runs if they’re too heavy for per-PR CI.

### P3 — Post-MVP / ADR-gated / “nice-to-have”

- [ ] **Consider group commit / WAL batching policies without weakening default durability** (Seen in 2/6: `DEEPSEEk32`, `GROKCODEFAST`; related I/O concerns in `QWEN`)
  - **Problem:** Synchronous fsync-on-every-commit is a major throughput limiter for small transactions, but is also the default durability guarantee.
  - **Impact:** High commit rates can bottleneck on fsync latency; however, weakening durability-by-default is unacceptable per project priorities.
  - **Actions to resolve:**
    - If ADR coverage is insufficient, write/update an ADR that defines allowed durability modes and guarantees (default remains fsync-on-commit).
    - Add an opt-in group commit mode (bounded delay) for users who accept the tradeoff.
    - Ensure crash-injection tests cover all durability modes.

- [ ] **Defer/explicitly document “cost-based optimization + full statistics” as post-MVP** (Seen in 4/6: `DEEPSEEk32`, `GROKCODEFAST`, `KIMIK25`, `QWEN`; also discussed in `MIMOV2`)
  - **Problem:** Multiple reviews request cost-based optimization and statistics collection; this is significant scope and complexity.
  - **Impact:** Better plans for complex queries, but not required to ship a correct MVP with basic heuristics.
  - **Actions to resolve:**
    - Track as a post-MVP initiative unless PRD requirements expand.
    - If implementing, define minimal statistics first (row counts, distinct counts) and add differential tests vs PostgreSQL for planner choices where relevant.
