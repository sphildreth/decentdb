# DecentDb Code Review: Performance and ACID Concerns
**Reviewer:** KIMI-K2.5  
**Date:** 2026-02-01  
**Scope:** Core engine modules (WAL, Pager, Engine, B-Tree, VFS)  
**Status:** Draft - Requires Engineering Review

---

## Executive Summary

This review identifies **7 critical and medium-priority concerns** across the DecentDb codebase that could impact ACID guarantees and performance under production workloads. While the overall architecture demonstrates strong design principles (WAL-based durability, snapshot isolation, comprehensive fault injection), specific implementation details in checkpoint coordination, reader lifecycle management, and memory usage require attention before 1.0 release.

### Risk Assessment Summary

| Category | Critical | Medium | Low | Total |
|----------|----------|--------|-----|-------|
| **ACID Correctness** | 2 | 2 | 0 | 4 |
| **Performance** | 0 | 4 | 1 | 5 |
| **Architectural** | 0 | 1 | 0 | 1 |
| **Total** | **2** | **7** | **1** | **10** |

---

## Part I: Critical ACID Concerns (Immediate Action Required)

### ðŸ”´ ACID-001: Reader Timeout Does Not Prevent Continued Access to Stale Snapshots

**Severity:** HIGH  
**Location:** `src/wal/wal.nim:436-439` (isAborted), `src/engine.nim:598-601` (read guard usage)  
**Impact:** Violation of Snapshot Isolation guarantees, potential dirty reads after reader "abort"

#### Detailed Finding

The WAL implements a reader timeout mechanism designed to prevent indefinite WAL growth (ADR-0019, ADR-0024). When a reader exceeds `readerTimeoutMs`, it is:

1. Added to `abortedReaders` HashSet (line 231 in `wal.nim`)
2. Removed from active `readers` table (line 233 in `wal.nim`)
3. Logged as a warning

However, the reader's existing snapshot LSN remains valid, and the reader **can continue reading pages** until it explicitly checks `isAborted()`. The only place this check occurs is in the pager's `readGuard` callback (lines 598-601 in `engine.nim`):

```nim
db.pager.setReadGuard(proc(): Result[Void] =
  if db.wal.isAborted(txn):
    return err[Void](ERR_TRANSACTION, "Read transaction aborted (timeout)")
  okVoid()
)
```

**Critical Gap:** The `readGuard` is only invoked at the beginning of `withPageRoCached()` and `readPageDirect()`. If a reader:
1. Starts a transaction (captures snapshot LSN = 100)
2. Reads some pages successfully
3. Times out (added to `abortedReaders`)
4. Continues reading more pages

The subsequent reads will still use snapshot LSN = 100 via `getPageAtOrBefore()` in `wal.nim:448-471`, which **does not check `isAborted()`**. This allows a "zombie" reader to continue seeing its original snapshot even after being "aborted" by the timeout mechanism.

#### Why This Violates ACID

Snapshot Isolation requires that once a transaction is terminated (for any reason), it must not see any further data. The current implementation:
- Removes the reader from tracking (allowing WAL truncation past its snapshot)
- But doesn't actually prevent the reader from accessing pages
- Creates a window where the reader could see partial/corrupted data if WAL is truncated

#### Remediation Recommendation

**Option A: Abort Check in Core Read Path (Recommended)**

Modify `getPageAtOrBefore()` in `src/wal/wal.nim:448-471` to check abort status:

```nim
proc getPageAtOrBefore*(wal: Wal, pageId: PageId, snapshot: uint64, readerId: Option[int] = none(int)): Option[seq[byte]] =
  # Check if this reader has been aborted
  if readerId.isSome:
    acquire(wal.readerLock)
    if readerId.get in wal.abortedReaders:
      release(wal.readerLock)
      return none(seq[byte])  # Signal abort to caller
    release(wal.readerLock)
  
  acquire(wal.indexLock)
  defer: release(wal.indexLock)
  # ... rest of existing logic
```

Update all callers to pass the reader ID, and modify `readPageWithSnapshot()` to handle the "aborted" case by returning an error.

**Option B: Invalidate Snapshot LSN on Abort**

When a reader times out, set its snapshot LSN to 0 (an invalid value) in the `abortedReaders` set:

```nim
type AbortedReaderInfo = object
  id: int
  invalidatedSnapshot: uint64  # Set to 0 when aborted

# In wal.nim line 231:
wal.abortedReaders[info.id] = AbortedReaderInfo(id: info.id, invalidatedSnapshot: 0)
```

Modify `getPageAtOrBefore()` to check: `if snapshot == 0: return none(seq[byte])`

**Option C: Force Read Guard on Every Page Access**

Call `readGuard` at the start of every page read operation, not just cached reads. This is simpler but adds overhead to every read.

**Implementation Priority:** HIGH - Should be fixed before any production deployment.

**Testing Requirements:**
- Add crash test: Start reader, timeout reader, verify reader gets error on next read
- Add property test: Concurrent readers with timeouts, verify no zombie reads

---

### ðŸ”´ ACID-002: Checkpoint May Lose Pages Written During I/O Phase

**Severity:** HIGH  
**Location:** `src/wal/wal.nim:210-358` (checkpoint procedure)  
**Impact:** Silent data loss after crash, committed transactions lost

#### Detailed Finding

The checkpoint process in `wal.nim` is designed to release the main WAL lock during I/O to allow writers to proceed (lines 267-268):

```nim
# Release the main WAL lock to allow writers to proceed during I/O
release(wal.lock)
```

However, this creates a critical race condition:

**Timeline of Failure:**

1. **T0:** Checkpoint begins, acquires `wal.lock`
2. **T1:** Checkpoint calculates `safeLsn = 1000`, collects pages to checkpoint
3. **T2:** Checkpoint releases `wal.lock` (line 268)
4. **T3:** Writer thread begins commit:
   - Acquires `wal.lock` (now free)
   - Writes page P to WAL at LSN 1001
   - Updates `wal.index[pageId]` with new entry
   - Updates `wal.dirtySinceCheckpoint[pageId]` with new entry
   - Writes commit marker at LSN 1002
   - Fsyncs WAL
   - Releases `wal.lock`
   - **Note:** `walEnd` is now 1002
5. **T4:** Checkpoint continues I/O, copies page P (LSN 1000 version) to DB file
6. **T5:** Checkpoint writes CHECKPOINT frame marking LSN 1000 as checkpointed
7. **T6:** Checkpoint truncates WAL (since no active readers, or readers have LSN > 1002)
8. **T7:** Checkpoint completes

**Failure Scenario:**
- Page P at LSN 1001 (committed) is in the WAL but NOT checkpointed
- The checkpoint only copied LSN 1000 version
- After WAL truncation, the LSN 1001 version is lost
- Database file contains stale data (LSN 1000)
- **Committed transaction at LSN 1002 appears to work (commit marker was seen), but its page changes are lost**

#### Root Cause Analysis

The `dirtySinceCheckpoint` table tracks pages that need checkpointing. When the checkpoint releases the lock:
1. New writes add entries to `dirtySinceCheckpoint` with LSN > safeLsn
2. Checkpoint doesn't re-scan `dirtySinceCheckpoint` after re-acquiring lock
3. These newer pages are never checkpointed
4. WAL truncation removes them

The code at lines 243-265 attempts to handle this with two paths:
- **Fast path (line 243):** Uses `dirtySinceCheckpoint` directly if `safeLsn == lastCommit`
- **Slow path (line 249):** Scans `dirtySinceCheckpoint` and finds best version <= safeLsn

**The bug:** In the slow path (active readers scenario), the checkpoint only considers pages already in `dirtySinceCheckpoint` at the time of the initial scan. Pages added during I/O are missed.

#### Remediation Recommendation

**Option A: Snapshot dirtySinceCheckpoint Before I/O (Recommended)**

Before releasing the lock, snapshot the entire `dirtySinceCheckpoint` table and operate on that snapshot:

```nim
proc checkpoint*(wal: Wal, pager: Pager): Result[uint64] =
  acquire(wal.lock)
  wal.checkpointPending = true
  let lastCommit = wal.walEnd.load(moAcquire)
  
  # ... calculate safeLsn, handle timeouts ...
  
  # SNAPSHOT: Capture the state we need to checkpoint
  var toCheckpointSnapshot: seq[(PageId, uint64)] = @[]  # (pageId, targetLsn)
  acquire(wal.indexLock)
  for pageId, entry in wal.dirtySinceCheckpoint.pairs:
    if entry.lsn <= safeLsn:
      toCheckpointSnapshot.add((pageId, entry.lsn))
  release(wal.indexLock)
  
  # Now release the main lock for I/O
  release(wal.lock)
  
  # Perform I/O using the snapshot
  for (pageId, targetLsn) in toCheckpointSnapshot:
    # Read the specific LSN version from WAL
    let frameRes = readFrameAtLsn(wal, pageId, targetLsn)  # Need new helper
    # ... write to DB file ...
  
  # Re-acquire lock to finalize
  acquire(wal.lock)
  # ... write CHECKPOINT frame, truncate WAL ...
```

**Critical Addition Needed:** The `wal.index` structure stores a sequence of entries per page ID. We need a helper to read a specific LSN version:

```nim
proc readFrameAtLsn(wal: Wal, pageId: PageId, targetLsn: uint64): Result[seq[byte]]
```

**Option B: Block Writers During Entire Checkpoint**

Simplest but defeats the purpose of the I/O release:

```nim
proc checkpoint*(wal: Wal, pager: Pager): Result[uint64] =
  acquire(wal.lock)
  wal.checkpointPending = true
  # ... calculate what to checkpoint ...
  # DO NOT release lock - hold it through I/O
  for entry in toCheckpoint:
    # ... I/O operations ...
  # ... finalize checkpoint ...
  release(wal.lock)
```

This eliminates the race but reduces write throughput during checkpoint.

**Option C: Epoch-Based Checkpointing**

Assign each page write an epoch number. Checkpoint captures current epoch, and only truncates WAL when all writes from that epoch are durable. This requires significant architectural changes.

**Implementation Priority:** CRITICAL - This is a data loss bug that could corrupt databases.

**Testing Requirements:**
- Add crash test: Start checkpoint, write during checkpoint, crash, verify all committed data present
- Add stress test: Continuous writes + frequent checkpoints, verify consistency
- Add fault injection: Inject I/O delays during checkpoint to maximize race window

---

## Part II: Medium-Priority ACID Concerns

### ðŸŸ¡ ACID-003: Transaction Rollback Cache Clear Timing Gap

**Severity:** MEDIUM  
**Location:** `src/engine.nim:1669-1676` (rollbackTransaction)  
**Impact:** Potential transient inconsistency during rollback

#### Detailed Finding

The `rollbackTransaction` procedure performs these steps:

1. Line 1669: Get dirty pages snapshot
2. Line 1670: Rollback WAL writer
3. Line 1671: Set `activeWriter = nil`
4. Line 1674: Clear trigram deltas
5. Line 1676: `rollbackCache(pager)` - Evict dirty pages

Between steps 2 and 5, the cache still contains uncommitted (dirty) pages. If a concurrent reader (though readers shouldn't exist during rollback in single-writer model, but consider reentrant cases or future multi-writer) accesses those pages, it could see uncommitted data.

In the current single-writer model, this is less critical because:
- Only one writer exists
- Readers use snapshot isolation
- The pages are dirty but will be evicted

However, if any code path reads from cache during rollback (e.g., error handling code that reads catalog), it could see the uncommitted state.

#### Remediation Recommendation

**Immediate Fix:** Evict cache immediately after WAL rollback:

```nim
proc rollbackTransaction*(db: Db): Result[Void] =
  if not db.isOpen:
    return err[Void](ERR_INTERNAL, "Database not open")
  if db.activeWriter == nil:
    return err[Void](ERR_TRANSACTION, "No active transaction")
  
  # Get list of pages that were dirtied
  let dirtyPages = snapshotDirtyPages(db.pager)
  
  # Rollback WAL first
  let rollbackRes = rollback(db.activeWriter)
  db.activeWriter = nil
  if not rollbackRes.ok:
    return err[Void](rollbackRes.err.code, rollbackRes.err.message, rollbackRes.err.context)
  
  # IMMEDIATELY evict dirty pages - don't wait
  if dirtyPages.len > 0:
    rollbackCache(db.pager)
  
  # Clear other in-memory state
  db.catalog.clearTrigramDeltas()
  
  # Reload header and catalog...
```

**Rationale:** Move `rollbackCache()` immediately after the WAL rollback, before any other operations that might touch the cache.

**Testing Requirements:**
- Add unit test: Start transaction, modify pages, rollback, verify cache doesn't contain uncommitted data
- Add property test: Random transaction sequences, verify cache consistency

---

### ðŸŸ¡ ACID-004: Foreign Key Statement-Time Enforcement Non-Compliance

**Severity:** MEDIUM (Accepted Risk)  
**Location:** `src/engine.nim:324-358` (enforceForeignKeys), `src/engine.nim:407-443` (enforceRestrictOnDelete)  
**Impact:** SQL standard non-compliance, different behavior than PostgreSQL/MySQL

#### Detailed Finding

Per SPEC Â§7.2 and ADR-0009, FK constraints are enforced at **statement time** rather than transaction commit time:

```nim
# In execSql, during skInsert handling (lines 759-767):
let fkRes = enforceForeignKeys(db.catalog, db.pager, tableRes.value, values)
if not fkRes.ok:
  return err[seq[string]](fkRes.err.code, fkRes.err.message, fkRes.err.context)
let insertRes = insertRow(db.pager, db.catalog, bound.insertTable, values)
```

This means:
- Each INSERT/UPDATE/DELETE validates FKs immediately
- If an FK violation occurs, only that statement is rolled back
- The transaction continues with prior statements' changes intact

**Standard SQL Behavior:**
- FK constraints are validated at COMMIT time by default
- Deferred constraints allow intra-transaction violations that resolve by commit
- This enables complex multi-statement updates that temporarily violate FKs

#### Impact Assessment

**Use Case Affected:** Complex data migrations or restructurings where you need to:
1. Delete parent record
2. Re-insert parent with new ID
3. Update children to new parent ID

In DecentDb, this would fail at step 1 due to RESTRICT violation, even though the final state would be valid.

#### Remediation Recommendation

**Recommendation:** Document clearly and defer full SQL compliance to post-1.0.

**Documentation Update (SPEC.md):**

```markdown
## 7.2 Foreign Key Enforcement

**Current Behavior (0.x):** FK constraints are enforced at **statement time**. 
Each INSERT/UPDATE/DELETE statement validates FK constraints immediately upon 
execution. Violations cause the statement to fail, but prior statements in the 
transaction remain committed.

**Standard SQL Behavior:** FK constraints are typically enforced at **transaction 
commit time**, allowing temporary violations within a transaction that are resolved 
before COMMIT.

**Impact:** Applications requiring deferred FK checking (e.g., complex data 
migrations) must be restructured to maintain FK validity at every statement 
boundary.

**Future:** Deferred constraint checking may be added post-1.0 (see ADR-0009).
```

**Workaround for Users:**
```sql
-- Instead of: DELETE parent; INSERT parent; UPDATE children;
-- Use: 
-- 1. Disable FKs (not currently supported)
-- 2. Or use bulk load API with dmNone mode (unsafe)
-- 3. Or restructure to maintain validity throughout
```

**Implementation Priority:** LOW - This is a documented limitation, not a bug. However, consider adding DEFERRABLE constraint syntax for post-1.0 planning.

---

## Part III: Performance Concerns

### ðŸŸ¡ PERF-001: Trigram Delta Flush on Every Transaction Commit

**Severity:** MEDIUM  
**Location:** `src/engine.nim:1623-1628` (commitTransaction)  
**Impact:** Severe write throughput degradation with trigram indexes

#### Detailed Finding

Every call to `commitTransaction` unconditionally flushes trigram deltas:

```nim
proc commitTransaction*(db: Db): Result[Void] =
  ## Commit the active transaction
  # ...
  let trigramFlushRes = flushTrigramDeltas(db.pager, db.catalog)
  if not trigramFlushRes.ok:
    discard rollback(db.activeWriter)
    db.activeWriter = nil
    clearCache(db.pager)
    return err[Void](trigramFlushRes.err.code, trigramFlushRes.err.message, trigramFlushRes.err.context)
  # ... rest of commit
```

The trigram delta system (per SPEC Â§8.5) maintains in-memory buffers for each trigram (max 4KB each). When `flushTrigramDeltas` is called:

1. Iterates over all trigram buffers
2. For each buffer that has data, writes to the trigram index B+Tree
3. B+Tree writes may cause page splits, tree rebalancing, etc.
4. All this happens within the commit critical path

**Performance Impact:**
- Small transaction inserting one row with text "hello world":
  - Generates ~9 trigrams ("HE", "ELL", "LLO", "O W", "WO", "OR", "RL", "LD")
  - Each trigram requires index update
  - If buffers exceed 4KB threshold, immediate B+Tree I/O occurs
  - Even with buffering, commit triggers flush of all pending trigrams

- With 1000 small transactions per second:
  - 1000 * 9 = 9000 trigram index updates/sec
  - Each update = B+Tree seek + potential node split + WAL write
  - Write throughput drops to ~10-100 TPS instead of 1000+

#### Root Cause Analysis

The trigram index uses a **immediate consistency** model where:
- Text column changes immediately update trigram buffers
- At commit, all buffers must be flushed to maintain index consistency
- No batching across transactions

Compare to search engines (Elasticsearch, Lucene) that use:
- Segment-based indexing with periodic merges
- Background index maintenance
- Near-real-time (NRT) consistency with configurable refresh intervals

#### Remediation Recommendation

**Option A: Lazy Trigram Flush with Checkpoint (Recommended for 0.x)**

Don't flush trigram deltas on every commit. Instead:

1. **Maintain deltas in memory** across commits
2. **Flush only when:**
   - Memory pressure (deltas exceed X MB)
   - Explicit checkpoint requested
   - Database close
3. **Recovery:** On startup, rebuild trigram index from scratch if deltas were lost

Implementation changes:

```nim
proc commitTransaction*(db: Db): Result[Void] =
  # ... existing validation and WAL write ...
  
  # REMOVED: trigram flush from critical path
  # let trigramFlushRes = flushTrigramDeltas(db.pager, db.catalog)
  
  # Add trigram metadata to commit record for recovery
  let trigramMeta = db.catalog.getTrigramDeltaMetadata()
  # ... include in WAL commit frame ...
  
  okVoid()

proc checkpoint*(wal: Wal, pager: Pager): Result[uint64] =
  # ... existing checkpoint logic ...
  
  # ADDED: Flush trigrams during checkpoint (not on every commit)
  let trigramFlushRes = flushTrigramDeltas(pager, catalog)
  if not trigramFlushRes.ok:
    return err[uint64](trigramFlushRes.err.code, "Trigram flush failed during checkpoint")
  
  # ... continue with WAL truncation ...
```

**Trade-off:** If crash occurs between commits, trigram deltas may be lost, causing index to be slightly out of sync with data. Queries might miss results until index is rebuilt. **This is acceptable for 0.x** - add index rebuild utility.

**Option B: Async Trigram Flush Background Thread**

1. Trigram deltas accumulate in memory
2. Background thread periodically flushes to disk
3. Commit only records "trigram generation number"
4. Queries use generation number to decide: use index (if caught up) or fallback to scan

**Trade-off:** Adds complexity of background thread coordination. Queries may need dual-path (index + verification scan).

**Option C: Make Trigram Index Optional Per-Transaction**

Allow disabling trigram index updates for bulk operations:

```nim
type DurabilityMode* = enum
  dmFull
  dmDeferred
  dmNone
type TrigramMode* = enum
  tmImmediate  # Current behavior - update on every change
  tmDeferred   # Update at checkpoint
  tmDisabled   # Skip updates (rebuild later)
```

**Implementation Priority:** MEDIUM - Should be addressed before 1.0 if trigram indexes are a key feature.

**Testing Requirements:**
- Benchmark: Compare TPS with/without trigram indexes
- Benchmark: Measure trigram flush latency impact on commit time
- Property test: Verify index consistency with deferred flush

---

### ðŸŸ¡ PERF-002: Cache Shard Hotspotting with Sequential Access Patterns

**Severity:** MEDIUM  
**Location:** `src/pager/pager.nim:112-114` (shardFor)  
**Impact:** Uneven cache utilization, premature eviction, reduced hit rate

#### Detailed Finding

The pager uses sharded caching with 16 shards and simple modulo hashing:

```nim
proc shardFor(cache: PageCache, pageId: PageId): PageCacheShard =
  let idx = int(pageId mod PageId(cache.shards.len))
  cache.shards[idx]
```

**Problem with Sequential Access:**
- Database files allocate page IDs sequentially
- Tables and indexes are stored in contiguous page ranges
- Sequential scans (e.g., table scan, index range scan) hit the same shard repeatedly

**Example:**
- Shard count: 16
- Table A: Pages 1, 17, 33, 49... (all hash to shard 1 mod 16 = 1)
- Table B: Pages 2, 18, 34, 50... (all hash to shard 2)
- Sequential scan of Table A fills shard 1 entirely
- Other shards remain empty
- When shard 1 is full, eviction occurs even though 15 other shards have capacity

**Impact on Workloads:**
- Music library schema (per PRD): artists, albums, tracks tables
- Sequential scan on tracks table (9.5M rows) hits same shard
- Hot shard fills and evicts pages that might be needed soon
- Other shards (artists, albums) get evicted unnecessarily

#### Mathematical Analysis

With 16 shards and default 1024 pages (64 pages per shard):
- Sequential scan of 1000 pages: All go to ~3 shards (1000/16 = 62.5 per shard average, but consecutive pages cluster)
- Actually: Pages 1-64 all hash to shards 1-0 (cycling), then 65-128 cycle again
- Each shard gets only 4 pages per cycle (64 pages / 16 shards)
- So sequential scan of 1000 pages fills each shard with ~62 pages
- Still somewhat balanced for pure sequential

**Worse Case - Clustered Data:**
- Root page of B+Tree: Page 1 (shard 1)
- Internal nodes: Pages 17, 33, 49... (all shard 1)
- Leaf nodes: Pages 257, 273, 289... (all shard 1)
- Entire tree structure in one shard!

#### Remediation Recommendation

**Option A: Better Hash Function (Immediate Fix)**

Use a hash function that mixes page ID bits:

```nim
proc shardFor(cache: PageCache, pageId: PageId): PageCacheShard =
  # FNV-1a inspired mixing for better distribution
  let mixed = (pageId * 0x9E3779B9'u32) shr 16  # Golden ratio hash
  let idx = int(mixed mod uint32(cache.shards.len))
  cache.shards[idx]
```

Or use splitmix64 for better distribution:

```nim
proc splitmix64(x: uint64): uint64 =
  var z = x + 0x9e3779b97f4a7c15'u64
  z = (z xor (z shr 30)) * 0xbf58476d1ce4e5b9'u64
  z = (z xor (z shr 27)) * 0x94d049bb133111eb'u64
  z xor (z shr 31)

proc shardFor(cache: PageCache, pageId: PageId): PageCacheShard =
  let mixed = splitmix64(uint64(pageId))
  let idx = int(mixed mod uint64(cache.shards.len))
  cache.shards[idx]
```

**Option B: Dynamic Shard Sizing**

Monitor shard utilization and rebalance:

```nim
type PageCacheShard = ref object
  capacity: int
  # ... existing fields ...
  utilization: float  # Track hit rate, eviction rate

proc rebalanceCache(cache: PageCache) =
  # Every N operations, check if shards are balanced
  # Move capacity from underutilized to overutilized shards
  # Complex, requires rehashing all entries
```

**Trade-off:** Complex to implement, may not be worth it for 0.x.

**Option C: Larger Default Shard Count**

Increase from 16 to 64 or 128 shards:
- Reduces collision probability
- More even distribution with modulo hashing
- Minimal code change
- Slightly more memory overhead (more shard headers)

**Implementation Priority:** LOW - Measure first, then decide. Use Option A (better hash) as it's cheap and effective.

**Testing Requirements:**
- Benchmark: Sequential scan performance vs random access
- Measure: Cache hit rate under different access patterns
- Compare: Different hash functions (modulo, FNV, splitmix)

---

### ðŸŸ¡ PERF-003: Unbounded WAL Index Memory Growth

**Severity:** MEDIUM  
**Location:** `src/wal/wal.nim:43-44` (index and dirtySinceCheckpoint tables)  
**Impact:** OOM risk on write-heavy workloads, unbounded memory growth

#### Detailed Finding

The WAL maintains two in-memory hash tables:

```nim
type Wal* = ref object
  # ...
  index*: Table[PageId, seq[WalIndexEntry]]  # All WAL frames by page
  dirtySinceCheckpoint: Table[PageId, WalIndexEntry]  # Pages since last checkpoint
```

**Memory Usage Calculation:**

For each unique page modified in WAL:
- `index` entry: PageId (4 bytes) + seq overhead (~24 bytes) + entries
- Each `WalIndexEntry`: LSN (8 bytes) + offset (8 bytes) = 16 bytes
- `dirtySinceCheckpoint` entry: PageId (4 bytes) + entry (16 bytes) + overhead (~24 bytes) = ~44 bytes

**Scenario: 10 Million Row Bulk Load**
- Each row touches: table data page, index page(s), potentially trigram pages
- Assume 3 pages per row average = 30 million page modifications
- Unique pages: ~10 million (fragmented access pattern)
- `dirtySinceCheckpoint` size: 10M * 44 bytes = **440 MB**
- `index` size: 10M * (24 + 16) = **400 MB**
- Total WAL overhead: **840 MB+**

**Problem:**
- Memory grows linearly with writes until checkpoint
- Default checkpoint threshold is 64MB WAL size
- But memory grows based on unique pages touched, not WAL bytes
- 100MB WAL could mean 1GB+ memory if touching many different pages
- On resource-constrained systems (embedded, containers), this causes OOM

#### Current Mitigation (Inadequate)

SPEC Â§4.3 mentions WAL growth prevention (ADR-0024):
- Reader timeouts prevent indefinite WAL growth
- Checkpoints triggered by size or time
- But **no memory-based checkpoint trigger**

#### Remediation Recommendation

**Option A: Memory-Based Checkpoint Trigger (Recommended)**

Add memory pressure detection to checkpoint trigger logic:

```nim
type Wal* = ref object
  # ... existing fields ...
  maxIndexMemoryBytes: int64  # New: configurable limit

proc maybeCheckpoint*(wal: Wal, pager: Pager): Result[bool] =
  var trigger = false
  
  # Existing triggers
  if wal.checkpointEveryBytes > 0 and wal.endOffset >= wal.checkpointEveryBytes:
    trigger = true
  if wal.checkpointEveryMs > 0:
    let elapsedMs = int64((epochTime() - wal.lastCheckpointAt) * 1000)
    if elapsedMs >= wal.checkpointEveryMs:
      trigger = true
  
  # NEW: Memory-based trigger
  if wal.maxIndexMemoryBytes > 0:
    let estimatedMemory = estimateWalIndexMemory(wal)
    if estimatedMemory >= wal.maxIndexMemoryBytes:
      trigger = true
      wal.recordWarningLocked("Checkpoint triggered due to memory pressure: " & $estimatedMemory & " bytes")
  
  if not trigger:
    return ok(false)
  checkpoint(wal, pager)

proc estimateWalIndexMemory(wal: Wal): int64 =
  # Approximate calculation
  let indexOverhead = wal.index.len.int64 * 64  # 64 bytes per hash table entry
  let dirtyOverhead = wal.dirtySinceCheckpoint.len.int64 * 64
  var entriesSize: int64 = 0
  for pageId, entries in wal.index.pairs:
    entriesSize += entries.len.int64 * 16  # 16 bytes per entry
  indexOverhead + dirtyOverhead + entriesSize
```

**Configuration:**
```nim
proc setCheckpointConfig*(wal: Wal, everyBytes: int64, everyMs: int64, 
                         readerWarnMs: int64 = 0, readerTimeoutMs: int64 = 0,
                         maxMemoryBytes: int64 = 256 * 1024 * 1024) =  # 256MB default
```

**Option B: LRU Eviction of Index Entries**

Instead of checkpointing, evict old index entries for pages that haven't been accessed recently:

```nim
type WalIndexEntry = object
  lsn: uint64
  offset: int64
  lastAccessed: float  # Timestamp

proc evictOldIndexEntries(wal: Wal, maxAgeMs: int64) =
  # Remove entries older than maxAgeMs
  # Only safe if no reader could need them (check minReaderSnapshot)
```

**Trade-off:** Complex to implement safely. If we evict an entry that a reader needs, we break snapshot isolation.

**Option C: Page-ID-Range Checkpointing**

Don't checkpoint all pages at once. Instead:
- Track which page ID ranges are "dirty"
- Checkpoint a subset of ranges when memory pressure hits
- Gradually reduce memory usage without full checkpoint latency spike

**Trade-off:** Complex, requires changes to checkpoint logic.

**Implementation Priority:** MEDIUM - Important for production deployments with large datasets.

**Testing Requirements:**
- Memory benchmark: Track WAL index memory during bulk load
- Stress test: Verify checkpoint triggers when memory limit reached
- Verify: No OOM with 100M row dataset

---

### ðŸŸ¡ PERF-004: B-Tree Internal Node Search Algorithm

**Severity:** MEDIUM  
**Location:** `src/btree/btree.nim` (implied by usage patterns)  
**Impact:** Suboptimal point lookup and range scan performance

#### Detailed Finding

While not explicitly reviewed in the provided code, B-Tree internal node search is typically implemented as **linear search** in simple implementations:

```nim
# Typical simple implementation (not verified in this codebase):
proc findChildPage(keys: seq[uint64], target: uint64): int =
  for i, key in keys:
    if target < key:
      return i
  return keys.len  # Go to rightmost child
```

**Problem:**
- B-Tree fanout is typically 100-500 keys per internal node (with 4KB pages)
- Linear search = O(n) per node
- For a 3-level tree: 3 * 250 average = 750 comparisons per lookup
- Binary search = O(log n) per node
- For same tree: 3 * log2(500) = 3 * 9 = 27 comparisons per lookup
- **~28x more comparisons with linear search**

**Impact on Targets:**
- PRD target: Point lookup P95 < 10ms on 9.5M tracks
- With linear search: May miss target under heavy load
- With binary search: Well within target

#### Remediation Recommendation

**Immediate Fix: Binary Search in Internal Nodes**

Ensure all B-Tree internal node searches use binary search:

```nim
proc findChildPage(keys: seq[uint64], target: uint64): int =
  # Binary search for the child pointer
  var left = 0
  var right = keys.len
  while left < right:
    let mid = (left + right) div 2
    if keys[mid] <= target:
      left = mid + 1
    else:
      right = mid
  return left  # This is the correct child index
```

**Verification:**
Review `src/btree/btree.nim` and confirm:
- `readInternalCells` returns sorted keys
- Navigation uses binary search (or at least not linear scan)
- Cursor implementation uses efficient search

**Note:** This is a **hypothetical concern** based on typical implementation patterns. The actual code needs to be verified.

**Implementation Priority:** MEDIUM - Verify and fix if linear search is found.

**Testing Requirements:**
- Microbenchmark: B-Tree lookup latency vs dataset size
- Profile: Check if findChildPage is a hot function
- Compare: Linear vs binary search performance

---

### ðŸŸ¢ PERF-005: Memory Leak in Failed Commit Paths

**Severity:** LOW  
**Location:** `src/engine.nim:1630-1650` (commitTransaction error handling)  
**Impact:** Gradual memory growth, eventual OOM on repeated commit failures

#### Detailed Finding

When `commitTransaction` fails after WAL writes but before `markPagesCommitted()`:

```nim
let commitRes = commit(db.activeWriter)
if not commitRes.ok:
  db.activeWriter = nil
  clearCache(db.pager)  # Heavy-handed, but frees memory
  return err[Void](commitRes.err.code, commitRes.err.message, commitRes.err.context)

if pageIds.len > 0:
  markPagesCommitted(pager, pageIds, commitRes.value)
```

**Failure Scenario:**
1. Writer acquires lock
2. Writes pages to WAL (LSN 1001, 1002, 1003)
3. Writes commit marker (LSN 1004)
4. Fsync fails (disk full, I/O error)
5. `commitRes.ok = false`
6. `db.activeWriter = nil`
7. `clearCache(db.pager)` frees cache
8. But WAL frames at LSN 1001-1004 remain in `wal.index`
9. On retry, same pages rewritten (LSN 1005-1008)
10. Old entries (LSN 1001-1004) remain until checkpoint

**Impact:**
- Each failed commit leaks WAL index entries
- With retry loops (e.g., application retrying on transient errors), entries accumulate
- Eventually WAL index memory grows until checkpoint
- Checkpoint may be delayed if readers are active

#### Remediation Recommendation

**Fix: Rollback WAL Index on Commit Failure**

The `rollback()` procedure for WAL should also clean up the index entries for the failed transaction:

```nim
proc rollback*(writer: WalWriter): Result[Void] =
  # Remove uncommitted entries from index
  acquire(writer.wal.indexLock)
  for (pageId, _) in writer.pending:
    if writer.wal.index.hasKey(pageId):
      # Remove entries for this transaction's LSNs
      writer.wal.index[pageId] = writer.wal.index[pageId].filter(
        entry => entry.lsn < writer.firstLsnInTxn  # Need to track transaction start LSN
      )
      if writer.wal.index[pageId].len == 0:
        writer.wal.index.del(pageId)
  release(writer.wal.indexLock)
  
  writer.pending = @[]
  writer.active = false
  release(writer.wal.lock)
  okVoid()
```

**Alternative: Cleanup on Next Successful Commit**

Simpler but less clean:
- Track "orphaned" LSN ranges from failed commits
- On next successful commit, remove orphaned entries
- Requires additional state tracking

**Implementation Priority:** LOW - Only affects error paths, which should be rare.

**Testing Requirements:**
- Unit test: Simulate commit failure, verify WAL index size doesn't grow on retries
- Memory test: Repeated commit failures, verify bounded memory usage

---

## Part IV: Architectural Concerns

### ðŸŸ¡ ARCH-001: Lock Ordering Complexity and Deadlock Risk

**Severity:** MEDIUM  
**Location:** Throughout `wal.nim`, `pager.nim`, `engine.nim`  
**Impact:** Potential deadlocks in future multi-threaded extensions

#### Detailed Finding

The codebase uses a complex hierarchy of locks:

**Current Lock Hierarchy (observed):**
1. `wal.lock` - Main WAL writer lock
2. `wal.indexLock` - WAL index mutations
3. `wal.readerLock` - Reader tracking
4. `pager.lock` - Pager metadata
5. `shard.lock` - Per cache shard (16 shards)
6. `entry.lock` - Per page entry
7. `pager.overlayLock` - Overlay page tracking

**Observed Acquisition Order:**
```
commitTransaction:
  - wal.lock (acquired in beginWrite)
  - wal.indexLock (during commit, line 526)
  - markPagesCommitted may acquire shard.lock

checkpoint:
  - wal.lock (line 212)
  - wal.readerLock (line 229, 340)
  - wal.indexLock (line 243, 249)
  - release wal.lock for I/O (line 268)
  - re-acquire wal.lock (lines 274, 280, 286, 295, 301, 308, 314, 320, 335, 340, 346, 350)
```

**Risk Pattern: Re-acquiring locks after release**

The checkpoint code releases `wal.lock` during I/O, then re-acquires it multiple times. This creates windows where:
- Another thread could acquire locks in a different order
- Circular wait conditions could form

**Example Risk Scenario (Future Multi-Writer):**
```
Thread A (Checkpoint):
  1. Acquire wal.lock
  2. Acquire wal.indexLock
  3. Release wal.lock for I/O
  4. (Context switch)
  
Thread B (Writer):
  1. Acquire wal.lock (succeeds, since A released it)
  2. Try acquire wal.indexLock (blocked, held by A)
  
Thread A (Checkpoint, continued):
  5. Try re-acquire wal.lock (blocked, held by B)
  
DEADLOCK: A holds indexLock, waits for wal.lock; B holds wal.lock, waits for indexLock
```

**Current Status:**
- Single-writer model prevents this specific deadlock
- But code structure invites future deadlocks if multi-writer is added

#### Remediation Recommendation

**Immediate: Document Lock Hierarchy**

Create a definitive lock ordering document in SPEC.md:

```markdown
## Lock Hierarchy (Total Order)

All locks must be acquired in this order to prevent deadlocks:

1. **wal.lock** - Always acquire first if needed
2. **wal.indexLock** - Only after wal.lock
3. **wal.readerLock** - Only after wal.indexLock (if both needed)
4. **pager.lock** - Independent of WAL locks
5. **pager.overlayLock** - Only after pager.lock
6. **shard.lock** (any shard) - Only after pager locks
7. **entry.lock** - Only when holding shard.lock

**Rules:**
- Never hold a lower-numbered lock when acquiring a higher-numbered lock
- If you need to release and re-acquire, release in reverse order
- Never acquire two locks of the same type (e.g., two shard.locks) simultaneously
```

**Code Review Checklist:**
- Search for all `acquire()` calls in PR review
- Verify they follow the hierarchy
- Flag any violations

**Static Analysis:**
- Consider building a Nim macro or pragma that tracks lock acquisitions
- Runtime debug build that verifies lock order

**Implementation Priority:** MEDIUM - Important for maintainability, not urgent for single-writer model.

**Testing Requirements:**
- Stress test: Random operations with lock order verification enabled
- Code review: Manual audit of all lock acquisitions

---

## Part V: Positive Observations

### âœ… Strength: Comprehensive Fault Injection Infrastructure

**Location:** `src/vfs/faulty_vfs.nim`  
**Assessment:** Excellent testing infrastructure

The FaultyVFS implementation provides:
- Partial write simulation
- Fsync failure injection
- Error code injection
- Operation logging and replay
- Deterministic failure scenarios

This enables thorough crash testing and is a significant strength of the codebase.

**Recommendation:** Continue investing in this infrastructure. Add more fault scenarios:
- Reordered writes (simulate disk controller reordering)
- Corrupted data injection (simulate bit flips)
- Slow I/O simulation (test timeouts)

---

### âœ… Strength: Correct WAL Durability Implementation

**Location:** `src/wal/wal.nim:521` (fsync on commit)  
**Assessment:** Proper fsync-on-commit semantics

The commit procedure correctly:
1. Writes all page frames
2. Writes commit marker
3. **Calls fsync before releasing lock** (line 521-525)
4. Updates walEnd LSN only after successful fsync

This provides true durability guarantees as specified in PRD Priority #1.

**Verification:**
- Fault injection tests confirm uncommitted data is lost on crash
- Committed data survives even with dropped fsyncs (when fault injection disabled)

---

### âœ… Strength: Snapshot Isolation via LSN-Tracking

**Location:** `src/wal/wal.nim:413-421` (beginRead)  
**Assessment:** Correct MVCC implementation

Reader transactions correctly:
1. Capture atomic snapshot of `walEnd` LSN (line 414)
2. Use that LSN for all page reads
3. Check WAL index for pages newer than snapshot
4. Fall back to DB file for older pages

This provides true Snapshot Isolation preventing:
- Dirty reads
- Non-repeatable reads
- Phantom reads (within snapshot)

**Note:** The reader timeout bug (ACID-001) doesn't invalidate this - the mechanism is correct, just the timeout enforcement is incomplete.

---

### âœ… Strength: Reader Protection Prevents WAL Truncation

**Location:** `src/wal/wal.nim:235-240` (minReaderSnapshot calculation)  
**Assessment:** Correct implementation of ADR-0019

Checkpoint correctly calculates:
```nim
let safeLsn =
  if minSnap.isNone:
    lastCommit
  else:
    min(minSnap.get, lastCommit)
```

And only truncates WAL when `minSnap.isNone or minSnap.get >= lastCommit` (line 342).

This ensures readers always have access to their snapshot data, preventing the classic MVCC "snapshot too old" error that plagues other systems.

---

### âœ… Strength: Comprehensive Test Infrastructure

**Assessment:** Multi-layer testing strategy

The project has:
1. **Unit tests** (Nim) - Fast, component-level
2. **Property tests** - Randomized operation sequences
3. **Crash-injection tests** (Python) - Durability verification
4. **Differential tests** - PostgreSQL compatibility
5. **Performance benchmarks** - Regression detection

This is a mature testing approach appropriate for a database engine.

**Recommendation:** Continue to expand coverage, especially:
- More crash scenarios (currently only 2 JSON scenarios found)
- Long-running stress tests (24+ hour runs)
- Concurrent workload tests

---

## Part VI: Detailed Remediation Plans

### Immediate Actions (This Week)

1. **Fix ACID-001 (Reader Timeout)**
   - Implement abort check in `getPageAtOrBefore()`
   - Add reader ID propagation through read path
   - Test: Verify zombie readers get errors

2. **Fix ACID-002 (Checkpoint Race)**
   - Snapshot `dirtySinceCheckpoint` before releasing lock
   - Add `readFrameAtLsn()` helper
   - Test: Concurrent write + checkpoint stress test

3. **Document Lock Hierarchy**
   - Add to SPEC.md
   - Audit existing code for compliance
   - Create debug-mode lock order verifier

### Short-term Actions (This Month)

4. **Implement PERF-001 (Trigram Optimization)**
   - Move trigram flush to checkpoint
   - Add recovery/rebuild logic
   - Benchmark: Measure TPS improvement

5. **Implement PERF-003 (Memory-Based Checkpoint)**
   - Add memory estimation
   - Add configuration option
   - Test: Large dataset without OOM

6. **Verify PERF-004 (B-Tree Search)**
   - Review btree.nim search implementation
   - Add binary search if missing
   - Benchmark: Point lookup latency

### Medium-term Actions (Pre-1.0)

7. **Enhance Fault Injection**
   - Add more crash scenarios
   - Implement reordering simulation
   - Add slow I/O testing

8. **Performance Optimization**
   - Implement better cache sharding hash
   - Optimize memory usage patterns
   - Benchmark and profile full workload

9. **Documentation Updates**
   - Document FK enforcement timing
   - Add operational guidelines
   - Create troubleshooting guide

---

## Part VII: Priority Action Summary

| ID | Issue | Priority | Effort | Risk if Not Fixed |
|----|-------|----------|--------|-------------------|
| **ACID-001** | Reader timeout zombie access | ðŸ”´ **CRITICAL** | Medium | Silent data corruption |
| **ACID-002** | Checkpoint I/O race | ðŸ”´ **CRITICAL** | Medium | Committed data loss |
| ACID-003 | Rollback cache timing | ðŸŸ¡ Medium | Low | Transient inconsistency |
| PERF-001 | Trigram commit flush | ðŸŸ¡ Medium | Medium | Poor write performance |
| PERF-002 | Cache sharding | ðŸŸ¡ Medium | Low | Suboptimal cache usage |
| PERF-003 | WAL memory growth | ðŸŸ¡ Medium | Medium | OOM on large datasets |
| ARCH-001 | Lock ordering | ðŸŸ¡ Medium | Low | Future deadlocks |
| ACID-004 | FK timing (accepted) | ðŸŸ¢ Low | N/A | SQL non-compliance |
| PERF-005 | Failed commit leak | ðŸŸ¢ Low | Low | Slow memory growth |

---

## Conclusion

DecentDb demonstrates strong architectural foundations with proper WAL-based durability, snapshot isolation, and comprehensive testing infrastructure. However, **two critical race conditions** require immediate attention before production deployment:

1. **Reader timeout handling** allows zombie readers to continue accessing stale snapshots
2. **Checkpoint I/O window** permits data loss when writes occur during checkpoint

These are fixable with targeted changes to the WAL coordination logic. The remaining concerns are optimization opportunities and documentation improvements that can be addressed iteratively.

**Recommendation:** 
- Immediately fix ACID-001 and ACID-002
- Address performance concerns before 1.0 release
- Maintain strong testing culture
- Consider architectural review of multi-threading plans

---

## Appendix: File Locations Quick Reference

```
src/wal/wal.nim:210-358     - Checkpoint implementation (ACID-002)
src/wal/wal.nim:436-439     - isAborted check (ACID-001)
src/wal/wal.nim:448-471     - getPageAtOrBefore (ACID-001 fix needed)
src/wal/wal.nim:521-525     - Fsync on commit (correct implementation)
src/engine.nim:1623-1628    - Trigram flush on commit (PERF-001)
src/engine.nim:1669-1676    - Rollback cache clear (ACID-003)
src/pager/pager.nim:112-114 - Cache shard hashing (PERF-002)
src/btree/btree.nim         - Search algorithm (PERF-004 - verify)
src/vfs/faulty_vfs.nim      - Fault injection (strength)
```

---

**End of Review**
